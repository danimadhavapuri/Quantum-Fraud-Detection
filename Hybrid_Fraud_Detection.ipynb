{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danimadhavapuri/Quantum-Fraud-Detection/blob/main/Hybrid_Fraud_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa4e07b8",
      "metadata": {
        "id": "fa4e07b8"
      },
      "source": [
        "# ðŸš€ Hybrid QML + ML Fraud Detection â€” Colab Notebook (Full Pipeline)\n",
        "**How to use:**  \n",
        "1. In Colab: *Runtime â†’ Change runtime type â†’ Python version â†’ 3.10* (important).  \n",
        "2. Then: *Runtime â†’ Restart runtime*.  \n",
        "3. Run all cells (Runtime â†’ Run all).\n",
        "\n",
        "This notebook is a self-contained demo: synthetic data â†’ graph â†’ Node2Vec embeddings â†’ classical baseline (XGBoost) â†’ QML classifier (PennyLane simulator) â†’ hybrid fusion â†’ evaluation & visualization. Designed to run on Colab CPU/GPU.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d88da7c4",
      "metadata": {
        "id": "d88da7c4"
      },
      "outputs": [],
      "source": [
        "# Install compatible packages\n",
        "# NOTE: If you are on Colab, please set Python version to 3.10 (Runtime -> Change runtime type -> Python version -> 3.10)\n",
        "# Then restart runtime and run this cell.\n",
        "!pip install --quiet numpy==1.23.5 pandas networkx scikit-learn matplotlib seaborn xgboost node2vec joblib pennylane pennylane-qiskit qiskit torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1\n",
        "print('Install finished')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c74a288",
      "metadata": {
        "id": "9c74a288"
      },
      "outputs": [],
      "source": [
        "# Imports and folders\n",
        "import os, random, math\n",
        "import numpy as np, pandas as pd, networkx as nx\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, roc_curve, auc\n",
        "import joblib\n",
        "import torch, torch.nn as nn\n",
        "os.makedirs('data/raw', exist_ok=True)\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "print('Imports loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10406a3e",
      "metadata": {
        "id": "10406a3e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# âœ… Synthetic transaction generator\n",
        "def generate_transactions(out_csv=\"data/processed/transactions.csv\",\n",
        "                          num_merchants=120, num_users=400, collusive_groups=4):\n",
        "    os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
        "\n",
        "    txns = []\n",
        "    merchants = [f\"M{i}\" for i in range(num_merchants)]\n",
        "    users = [f\"U{i}\" for i in range(num_users)]\n",
        "    start = datetime.now() - timedelta(days=30)\n",
        "\n",
        "    # Normal user â†’ merchant transactions\n",
        "    for _ in range(100):\n",
        "        s = random.choice(users)\n",
        "        r = random.choice(merchants)\n",
        "        t = start + timedelta(minutes=random.randint(0, 60*24*30))\n",
        "        amt = round(random.uniform(10, 2000), 2)\n",
        "        txns.append((s, r, t.isoformat(), amt))\n",
        "\n",
        "    # Random merchant â†’ merchant transactions\n",
        "    for _ in range(50):\n",
        "        a, b = random.sample(merchants, 2)\n",
        "        t = start + timedelta(minutes=random.randint(0, 60*24*30))\n",
        "        amt = round(random.uniform(5, 5000), 2)\n",
        "        txns.append((a, b, t.isoformat(), amt))\n",
        "\n",
        "    # Collusive merchant groups\n",
        "    collusive_nodes = set()\n",
        "    for g in range(collusive_groups):\n",
        "        size = random.randint(3, 5)\n",
        "        group = random.sample(merchants, size)\n",
        "        collusive_nodes.update(group)\n",
        "        base_time = start + timedelta(days=random.randint(0, 29))\n",
        "        for _ in range(20):\n",
        "            for i in range(len(group)):\n",
        "                a = group[i]\n",
        "                b = group[(i+1) % len(group)]\n",
        "                t = base_time + timedelta(minutes=random.randint(0, 60*6))\n",
        "                amt = round(random.uniform(1000, 20000), 2)\n",
        "                txns.append((a, b, t.isoformat(), amt))\n",
        "\n",
        "    df = pd.DataFrame(txns, columns=[\"sender\", \"receiver\", \"timestamp\", \"amount\"])\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"âœ… Saved {len(df)} transactions to {out_csv}. Collusive nodes approx: {len(collusive_nodes)}\")\n",
        "    return df\n",
        "\n",
        "# ðŸ‘‰ Run Step 1\n",
        "df = generate_transactions()\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbca30ab",
      "metadata": {
        "id": "cbca30ab"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "# âœ… Graph builder\n",
        "def build_graph(csv_path=\"data/processed/transactions.csv\", out_g=\"data/processed/txn_graph.pkl\"):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        a, b, amt, t = row[\"sender\"], row[\"receiver\"], row[\"amount\"], pd.to_datetime(row[\"timestamp\"])\n",
        "        if G.has_edge(a, b):\n",
        "            G[a][b][\"count\"] += 1\n",
        "            G[a][b][\"amount_sum\"] += amt\n",
        "            G[a][b][\"last_ts\"] = max(G[a][b][\"last_ts\"], t)\n",
        "        else:\n",
        "            G.add_edge(a, b, count=1, amount_sum=amt, last_ts=t)\n",
        "\n",
        "    # Save graph object\n",
        "    with open(out_g, \"wb\") as f:\n",
        "        pickle.dump(G, f)\n",
        "\n",
        "    print(f\"âœ… Graph built: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "    return G\n",
        "\n",
        "def load_graph(path=\"data/processed/txn_graph.pkl\"):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# ðŸ‘‰ Run Step 2\n",
        "G = build_graph()\n",
        "print(\"Nodes:\", G.number_of_nodes())\n",
        "print(\"Edges:\", G.number_of_edges())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2709c789",
      "metadata": {
        "id": "2709c789"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def extract_features(G, out_csv=\"data/processed/features.csv\"):\n",
        "    nodes = list(G.nodes())\n",
        "    pagerank = nx.pagerank(G, alpha=0.85)\n",
        "    indeg = dict(G.in_degree())\n",
        "    outdeg = dict(G.out_degree())\n",
        "\n",
        "    # âœ… Safer cycle feature: count triangles (fast)\n",
        "    cycle_counts = defaultdict(int)\n",
        "    for n in nodes:\n",
        "        # neighbors that form 2-step paths\n",
        "        for nbr in G.successors(n):\n",
        "            for nnbr in G.successors(nbr):\n",
        "                if G.has_edge(nnbr, n):  # triangle detected\n",
        "                    cycle_counts[n] += 1\n",
        "\n",
        "    # âœ… Strongly connected component size\n",
        "    scc = {n: len(c) for c in nx.strongly_connected_components(G) for n in c}\n",
        "\n",
        "    rows = []\n",
        "    for n in nodes:\n",
        "        rows.append({\n",
        "            \"node\": n,\n",
        "            \"in_deg\": indeg.get(n, 0),\n",
        "            \"out_deg\": outdeg.get(n, 0),\n",
        "            \"pagerank\": pagerank.get(n, 0.0),\n",
        "            \"cycle_count\": cycle_counts.get(n, 0),  # lighter cycle metric\n",
        "            \"scc_size\": scc.get(n, 1)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df[\"label\"] = (df[\"cycle_count\"] > 0).astype(int)  # mark cyclic nodes as fraud-like\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"âœ… Saved features for {len(df)} nodes to {out_csv}\")\n",
        "    return df\n",
        "\n",
        "# Ensure graph is available\n",
        "try:\n",
        "    G\n",
        "except NameError:\n",
        "    # if not in memory, reload from pickle\n",
        "    G = load_graph(\"data/processed/txn_graph.pkl\")\n",
        "feat = extract_features(G)\n",
        "feat.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install node2vec"
      ],
      "metadata": {
        "id": "U-TMYs_7PKr5"
      },
      "id": "U-TMYs_7PKr5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85fa713e",
      "metadata": {
        "id": "85fa713e"
      },
      "outputs": [],
      "source": [
        "# Node2Vec embeddings (CPU-friendly)\n",
        "from node2vec import Node2Vec\n",
        "nodes = list(G.nodes())\n",
        "G_u = G.to_undirected()\n",
        "node2vec = Node2Vec(G_u, dimensions=32, walk_length=20, num_walks=100, workers=2)\n",
        "w2v_model = node2vec.fit(window=10, min_count=1)\n",
        "emb = np.array([w2v_model.wv[n] for n in nodes])\n",
        "np.save('data/processed/embeddings.npy', emb)\n",
        "pd.DataFrame({'node': nodes}).to_csv('data/processed/node_index.csv', index=False)\n",
        "print('Saved embeddings shape:', emb.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "334e7e7c",
      "metadata": {
        "id": "334e7e7c"
      },
      "outputs": [],
      "source": [
        "# Classical baseline (XGBoost)\n",
        "from xgboost import XGBClassifier\n",
        "X = feat[['in_deg','out_deg','pagerank','cycle_count','scc_size']].values\n",
        "y = feat['label'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "clf = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss')\n",
        "clf.fit(X_train, y_train)\n",
        "preds = clf.predict(X_test); probs = clf.predict_proba(X_test)[:,1]\n",
        "print('XGBoost results:'); print(classification_report(y_test, preds))\n",
        "print('XGBoost AUC:', roc_auc_score(y_test, probs))\n",
        "joblib.dump(clf, 'data/processed/xgb_model.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pennylane"
      ],
      "metadata": {
        "id": "0AuqBF85Qn62"
      },
      "id": "0AuqBF85Qn62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QML classifier using PennyLane (simulator)\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pennylane as qml\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Load embeddings and labels ---\n",
        "emb_all = np.load('data/processed/embeddings.npy')\n",
        "node_index = pd.read_csv('data/processed/node_index.csv')['node'].tolist()\n",
        "\n",
        "# Load labels (adjust if labels are stored elsewhere)\n",
        "feat = pd.read_csv(\"data/processed/features.csv\")  # features file has 'node' and 'label'\n",
        "label_map = dict(zip(feat['node'], feat['label']))\n",
        "labels_all = np.array([label_map.get(n, 0) for n in node_index])\n",
        "\n",
        "# --- Dimensionality reduction for QML input ---\n",
        "n_qubits = 4\n",
        "pca = PCA(n_components=n_qubits)\n",
        "Xp = pca.fit_transform(emb_all)\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(-np.pi, np.pi))\n",
        "Xp = scaler.fit_transform(Xp)\n",
        "\n",
        "# Train/test split\n",
        "Xtr, Xte, ytr, yte = train_test_split(\n",
        "    Xp, labels_all, test_size=0.2, random_state=42, stratify=labels_all\n",
        ")\n",
        "\n",
        "# --- Quantum device ---\n",
        "dev = qml.device('default.qubit', wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def circuit(inputs, weights):\n",
        "    # Encode classical data\n",
        "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    # Entangling layers\n",
        "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "    # Return expectation values (vector of length n_qubits)\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# Define trainable weights\n",
        "weight_shapes = {\"weights\": (3, n_qubits, 3)}  # 3 layers\n",
        "\n",
        "# Wrap QNode into Torch layer (no batch_dim argument)\n",
        "qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)\n",
        "\n",
        "# --- QML Classifier ---\n",
        "class QMLClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.qlayer = qlayer\n",
        "        self.fc = nn.Linear(n_qubits, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.qlayer(x)\n",
        "        return torch.sigmoid(self.fc(x)).squeeze(-1)\n",
        "\n",
        "# Instantiate model\n",
        "model = QMLClassifier()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "# Convert data to tensors\n",
        "Xtr_t = torch.tensor(Xtr, dtype=torch.float32)\n",
        "ytr_t = torch.tensor(ytr, dtype=torch.float32)\n",
        "Xte_t = torch.tensor(Xte, dtype=torch.float32)\n",
        "yte_t = torch.tensor(yte, dtype=torch.float32)\n",
        "\n",
        "# --- Training loop ---\n",
        "epochs = 20\n",
        "batch = 16  # smaller batch to save RAM\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    perm = torch.randperm(Xtr_t.size(0))\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i in range(0, Xtr_t.size(0), batch):\n",
        "        idx = perm[i:i+batch]\n",
        "        xb = Xtr_t[idx]\n",
        "        yb = ytr_t[idx]\n",
        "\n",
        "        opt.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = loss_fn(out, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    total_loss /= Xtr_t.size(0)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds_val = model(Xte_t).detach().numpy()\n",
        "    try:\n",
        "        auc = roc_auc_score(yte, preds_val)\n",
        "    except Exception:\n",
        "        auc = float(\"nan\")\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == epochs - 1:\n",
        "        print(f\"Epoch {epoch} loss={total_loss:.4f} val_auc={auc:.4f}\")\n",
        "\n",
        "# --- Save artifacts ---\n",
        "torch.save(\n",
        "    {\"state\": model.state_dict(), \"pca\": pca, \"scaler\": scaler},\n",
        "    \"data/processed/qml_model.pt\"\n",
        ")\n",
        "print(\"âœ… Saved QML artifacts\")\n"
      ],
      "metadata": {
        "id": "2K0zOQSkVOKz"
      },
      "id": "2K0zOQSkVOKz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "614863f9",
      "metadata": {
        "id": "614863f9"
      },
      "outputs": [],
      "source": [
        "# Fusion: combine classical + QML probabilities using a simple logistic regression meta-model\n",
        "with torch.no_grad():\n",
        "    Xp_all = pca.transform(emb_all)\n",
        "    Xp_all = scaler.transform(Xp_all)\n",
        "    qml_probs_all = model(torch.tensor(Xp_all, dtype=torch.float32)).numpy()\n",
        "\n",
        "xgb_probs_all = clf.predict_proba(feat[['in_deg','out_deg','pagerank','cycle_count','scc_size']].values)[:,1]\n",
        "\n",
        "X_meta = np.vstack([xgb_probs_all, qml_probs_all]).T\n",
        "y_meta = labels_all\n",
        "X_m_tr, X_m_te, y_m_tr, y_m_te = train_test_split(X_meta, y_meta, test_size=0.2, random_state=42, stratify=y_meta)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "meta = LogisticRegression().fit(X_m_tr, y_m_tr)\n",
        "preds_meta = meta.predict(X_m_te); probs_meta = meta.predict_proba(X_m_te)[:,1]\n",
        "print('Meta model results:'); print(classification_report(y_m_te, preds_meta))\n",
        "print('Meta AUC:', roc_auc_score(y_m_te, probs_meta))\n",
        "joblib.dump(meta, 'data/processed/meta_model.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "922ccf33",
      "metadata": {
        "id": "922ccf33"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "\n",
        "# --- Top suspicious nodes visualization ---\n",
        "feat['xgb_score'] = xgb_probs_all\n",
        "feat['qml_score'] = qml_probs_all\n",
        "feat['meta_score'] = meta.predict_proba(X_meta)[:,1]\n",
        "\n",
        "top_nodes = feat.sort_values('meta_score', ascending=False).head(15)['node'].tolist()\n",
        "H = G.subgraph(top_nodes).copy()\n",
        "pos = nx.spring_layout(H, seed=42)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "nx.draw(H, pos, with_labels=True, node_size=450, node_color='red', font_size=8)\n",
        "plt.title('Top suspicious nodes (meta model)')\n",
        "plt.show()\n",
        "\n",
        "# --- Confusion matrix ---\n",
        "cm = confusion_matrix(y_m_te, preds_meta)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Meta Model')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# --- ROC curve ---\n",
        "fpr, tpr, _ = roc_curve(y_m_te, probs_meta)\n",
        "roc_auc_val = auc(fpr, tpr)  # use different name to avoid conflict\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {roc_auc_val:.2f}')\n",
        "plt.plot([0,1], [0,1], 'r--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC - Meta Model')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa9de269",
      "metadata": {
        "id": "fa9de269"
      },
      "outputs": [],
      "source": [
        "# Zip results for download\n",
        "!zip -r /content/qml_fraud_demo_results.zip data -q\n",
        "from google.colab import files\n",
        "files.download('/content/qml_fraud_demo_results.zip')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}